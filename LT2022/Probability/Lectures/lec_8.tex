\lecture{8}{5 Feb. 2022}{}
\subsection{Discrete Probability Distributions}
We first start with distributions with \(\Omega\) finite.
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
\subsubsection{Bernoulli Distribution (``biased coin toss'')}
We have \(X \sim \mathrm{Bern}(p)\) with \(p \in [0,1]\), and
\begin{align*}
    \Ima(X) &= \{0,1\}\\
    p_X(1) &= \probability{}{X = 1} = p\\
    p_X(0) &= \probability{}{X = 0} = 1-p.
\end{align*}
\begin{example}
    \(\textbf{1}_A \sim \mathrm{Bern}(p)\) with \(p = \probability{}{A}\).
\end{example}
\subsubsection{Binomial Distribution}
We have \(X \sim \mathrm{Bin}(n,p)\) with \(n \in \mathbb{Z}^+, p \in [0,1]\). (``Toss coin \(n\) times, count number of heads'') We have
\begin{align*}
    \Ima(X) &= \{0,1, \ldots, n\}\\
    p_X(k) &= \probability{}{X = k} = \binom{n}{k}p^k (1-p)^{n-k}.
\end{align*}
Do check that \(\sum\limits_{k=0}^{n} p_X(k) = 1\) by binomial expansion.
Next, we consider \(\Omega=\mathbb{N}\). ("Ways of choosing a random integer")
\subsubsection{Geometric Distribution (``Waiting for success'')}
We have \(X \sim \mathrm{Geom}(p)\) with \(p \in (0,1]\). (``Toss a coin with \(\probability{}{\text{head}} =p\) until a head appears. Count how many trials were needed'') So
\begin{align*}
    \Ima(X) &= \{1,2 \ldots\}\\
    p_X(k) &= \probability{}{(n-1)\text{ failures, then success on last}} = (1-p)^{k-1}p. 
\end{align*}
Indeed, we have
\[
    \sum\limits_{k\geq 1}(1-p)^{k-1}p = p \sum_{\ell\geq 0}(1-p)^\ell = \frac{p}{1 - (1-p)} = 1.
\]

Alternatively, we can count how many failures before a success. So
\begin{align*}
    \Ima(Y) &= \{0,1,2, \ldots\}\\
    p_Y(k) &= \probability{}{k \text{ failures, then success on next}} = (1-p)^k p.
\end{align*}
Similarly, we have
\[
    \sum\limits_{k\geq 0}(1-p)^{k}p = 1.
\]
\subsubsection{Poisson Distribution}
We have \(X \sim \mathrm{Po}(\lambda)\) (or \(\mathrm{Poi}(\lambda)\) with parameter \(\lambda\)), and
\begin{align*}
    \Ima(X) &= \{0,1,2, \ldots\}\\
    p_X(k) &= e^{-\lambda}\frac{\lambda^k}{k!}.
\end{align*}
Note that \(\sum_{k\geq 0} \probability{}{X = k} = e^{-\lambda} \sum_{k\geq 0}\frac{\lambda^k}{k!}=e^{-\lambda}e^{\lambda} = 1\).

Motivation: Consider \(X_n \sim \mathrm{Bin}(n, \frac{\lambda}{n})\), we split time interval \([0, \lambda]\) into \(n\) small intervals. If the probability of arrival in each interval is \(p\), and independent across intervals. The total number of arrivals is \(X_n\), and note by fixing \(k\) and taking \(n \to \infty\),
\begin{align*}
    \probability{}{X_n = k} &= \binom{n}{k}(\frac{\lambda}{n})^k(1-\frac{\lambda}{n})^{n-k}\\
    &= \frac{n!}{n^k(n-k)!} \times \frac{\lambda^k}{k!}\times (1-\frac{\lambda}{n})^n \times (1-\frac{\lambda}{n})^{-k}\\
    &\to 1 \times \frac{\lambda^k}{k!} \times e^{-\lambda} \times 1 = e^{-\lambda}\frac{\lambda^k}{k!}.
\end{align*}
\subsection{More Than One RV}
Motivation: Roll a die, and the outcome is \(X \in \{1,2,3,4,5,6\}\). If we consider the events
\[
    A = \{1 \text{ or } 2\},~B = \{1 \text{ or } 2 \text{ or } 3\},~C = \{1 \text{ or } 3 \text{ or } 5\}.
\]
We have
\[
    \textbf{1}_A \sim \mathrm{Bern}(\frac{1}{3}),~\textbf{1}_B \sim \mathrm{Bern}(\frac{1}{2}),~\textbf{1}_C \sim \mathrm{Bern}(\frac{1}{2}) .
\]
Note \(\textbf{1}_A \leq \textbf{1}_B\) for all outcomes, but \(\textbf{1}_A \leq \textbf{1}_C\) is not true for all outcomes.
\begin{definition}
    \(X_1, \ldots, X_n\) discrete RVs, then we say \(X_1, \ldots, X_n\) are \textit{independent} if
    \[
        \probability{}{X_1 = x_1, \ldots, X_n = x_n} = \probability{}{X_1 = x_1} \cdots \probability{}{X_n = x_n}\quad \forall x_1, \ldots, x_n \in \mathbb{R}.
    \]
\end{definition}
\begin{remark}
    It suffices to check that \(\forall x_i \in \Ima(X_i)\).
\end{remark}
\begin{example}
    \(X_1, \ldots, X_n\) independent RVs each with the \(\mathrm{Bern}(p)\) distribution. We study \(S_n = X_1 + \cdots + X_n\). Then
    \begin{align*}
        \probability{}{S_n = k} &= \sum_{\substack{x_1 + \cdots + x_n = k\\x_i \in \{0,1\}}} \probability{}{X_1 = x_1, \ldots, X_n = x_n}\\
        &= \sum_{\substack{x_1 + \cdots + x_n = k\\x_i \in \{0,1\}}} \probability{}{X_1 = x_1}\cdots\probability{}{X_n = x_n} \text{ by independence}\\
        &= \sum_{\substack{x_1 + \cdots + x_n = k\\x_i \in \{0,1\}}}p^{\left\vert \{i\mid x_i = 1\} \right\vert}(1-p)^{\left\vert \{i\mid x_i = 0\} \right\vert}\\
        &= \sum_{\substack{x_1 + \cdots + x_n = k\\x_i \in \{0,1\}}}p^k(1-p)^{n-k}\\
        &= \binom{n}{k}p^k (1-p)^{n-k}.
    \end{align*}
    So \(S_n \sim \mathrm{Bin}(n, k)\).
\end{example}
\begin{example}
    Consider the uniform permutation \((\sigma(1), \ldots, \sigma(n)\) of the integers \(1, 2, \ldots, n\). We claim that \(\sigma(1)\) and \(\sigma(2)\) are not independent.

    It suffices to find \(i_1, i_2\) such that
    \[
        \probability{}{\sigma(1) = i_1, \sigma(2) = i_2} \neq \probability{}{\sigma(1)=i_1}\probability{}{\sigma(2) = i_2}.
    \]
    For example,
    \[
        \probability{}{\sigma(1) = 1, \sigma(2) = 1} = 0 \neq \probability{}{\sigma(1)=1}\probability{}{\sigma(2) = 1} = \frac{1}{n}\cdot \frac{1}{n}.
    \]
\end{example}
We also have that if \(X_1, \ldots, X_n\) are independent, \(\forall A_1, \ldots, A_n \in \mathbb{R}\) countable,
\[
    \probability{}{X_1 \in A_1, \ldots, X_n \in A_n} = \probability{}{X_1 \in A_1} \cdots \probability{}{X_n \in A_n}.
\]