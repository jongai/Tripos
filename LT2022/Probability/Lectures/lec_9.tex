\lecture{9}{8 Feb. 2022}{}
\subsection{Expectation}
If we have \((\Omega, \mathcal{F}, \mathbb{P})\) and \(X\) a discrete RV. For now, \(X\) only takes non-negative values. "\(X\geq 0\)"
\begin{definition}{}{}
    \textit{The expectation of \(X\)} (or \textit{expected value} or \textit{mean}).
    \[
        \mathbb{E}[X] = \sum_{x\in \ima(X)} x \mathbb{P}(X = x) = \sum_{\omega \in\Omega}X(\omega)\mathbb{P}(\{\omega\}).
    \]
    The latter definition is only used in a later proof once.
\end{definition}
\begin{remark}
    Informally, this is the ``average of values taken by \(X\), weighted by \(p_X\)''.
\end{remark}
\begin{example}
    If we have \(X\) uniform on \(\{1, 2, \dots, 6\}\) (e.g., a die), we have
    \[
        \mathbb{E}[X] = \frac{1}{6}\times 1 + \frac{1}{6} \times 2 + \dots + \frac{1}{6} \times 6 = 3.5.
    \]
    Note that \(\mathbb{E}[X] \notin \ima(X)\).
\end{example}
\begin{example}
    If \(X \sim \mathrm{Bin}(n,p)\). We have
    \[
        \mathbb{E}[X] = \sum_{k=0}^{n} k \mathbb{P}(X = k) = \sum_{k=0}^{n} k\binom{n}{k}p^k(1-p)^{n-k}.
    \]
    Note that
    \[
        k\binom{n}{k}= \frac{k \times n!}{k! \times (n - k)!} = \frac{n!}{(k-1)!(n-k)!} = \frac{n \times (n - 1)!}{(k-1)! \times (n-k)!} = n \binom{n-1}{k-1}.
    \]
    So we have
    \begin{align*}
        \mathbb{E}[X] &= n \sum_{k=1}^{n} \binom{n - 1}{k - 1} p^k (1-p)^{n - k}\\
        & = np \sum_{k=1}^{n} \binom{n - 1}{k - 1} p^{k-1}(1-p)^{(n - 1) - (k -1)}\\
        & = np \sum_{\ell=0}^{n - 1} \binom{n - 1}{\ell}p^\ell (1 - p)^{(n - 1) - \ell}\\
        & = np (p + (1 - p))^{n - 1}\\
        & = np.
    \end{align*}
    \begin{note}
        We would like to say that
        \[
            \mathbb{E}[\mathrm{Bin}(n, p)] = \mathbb{E}[\mathrm{Bern}(p)] + \dots + \mathbb{E}[\mathrm{Bern}(p)].
        \]
        We will show this later in the lecture.
    \end{note}
\end{example}
\begin{example}
    If \(X \sim \mathrm{Poisson}(\lambda)\),
    \begin{align*}
        \mathbb{E}[X] &= \sum_{k \geq 0} k \mathbb{P}(X = k) = \sum_{k \geq 0} k \cdot e^{-\lambda}\frac{\lambda^k}{k!}\\
        &= \sum_{k \geq 1} e^{-\lambda}\frac{\lambda^k}{(k - 1)!}\\
        &= \lambda \sum_{k \geq 1} e^{-\lambda}\frac{\lambda^{k - 1}}{(k - 1)!}\\
        &= \lambda \sum_{\ell \geq 0} e^{-\lambda} \frac{\lambda^{\ell}}{\ell!}\\
        &= \lambda.
    \end{align*}
    \begin{note}
        We would like to say that
        \[
            \mathbb{E}[\mathrm{Poisson} (\lambda)] \approx \mathbb{E}[\mathrm{Bin}(n, \frac{\lambda}{n})] = \lambda.
        \]
        But it is not true in general that \(\mathbb{P}(X_n = k) \approx \mathbb{P}(X = k) \implies \mathbb{E}[X_n] \approx \mathbb{E}[X]\).
    \end{note}
\end{example}
For a general \(X\) (not necessarily \(X \geq 0\)),
\[
    \mathbb{E}[X] = \sum_{x \in \ima(X)} x \mathbb{P}(X = x)
\]
unless \(\sum_{\substack{x > 0\\ x \in \ima(X)}}x \mathbb{P}(X = x) = + \infty\) and \(\sum_{\substack{x < 0\\ x \in \ima(X)}}x \mathbb{P}(X = x) = - \infty\), then we say \(\mathbb{E}[X]\) is not defined. (because we don't want to do arithmetic with infinity)

If only one of them holds, we say that \(\mathbb{E}[X]\) is \(+ \infty\) and \(- \infty\) respectively. (some people say that it is undefined, but the lecturer disagrees with it) If neither of them hold, we say \(X\) is integrable.

\begin{example}
    Most examples in the course are integrable except the following. Let
    \[
        \mathbb{P}(x = n) = \frac{6}{\pi^2} \times \frac{1}{n^2}. \qquad x \geq 1
    \]
    Note that \(\sum \mathbb{P}(X = n) = 1\). Then
    \[
        \mathbb{E}[X] = \sum \frac{6}{\pi^2} \times\frac{1}{n} = + \infty.
    \]

    If instead, let
    \[
        \mathbb{P}(X = n) = \frac{3}{\pi^2} \times\frac{1}{n^2}.\qquad n \in \mathbb{Z}\setminus \{0\}
    \]
    Then \(\mathbb{E}[X]\) is not defined.
\end{example}
\begin{example}
    \(\mathbb{E}[\mathbf{1}_A] = \mathbb{P}(A)\).
\end{example}
\begin{property}
\begin{enumerate}
    \item If \(X \geq 0\), then \(\mathbb{E}[X] \geq 0\) with equality if and only if \(\mathbb{P}(X = 0) = 1\).
    \begin{proof}
        \(\mathbb{E}[X] = \sum_{\substack{x \in \ima(X)\\ x\neq 0}} x \mathbb{P}(X = x)\).
    \end{proof}
    \item If \(\lambda, c \in \mathbb{R}\), then
    \begin{enumerate}
        \item \(\mathbb{E}[X + c] = \mathbb{E}[x] + c\);
        \item \(\mathbb{E}[\lambda X] = \lambda\mathbb{E}[X]\).
    \end{enumerate}
    \item \begin{enumerate}
        \item For \(X, Y\) random variables (both integrable) on same probability space,
        \[
            \mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y].
        \]
        \item In fact, for \(\lambda, \mu \in \mathbb{R}\),
        \[
            \mathbb{E}[\lambda X + \mu Y] = \lambda \mathbb{E}[X] + \mu\mathbb{E}[Y].
        \]
        \begin{proof}
            We have
            \begin{align*}
                \mathbb{E}[\lambda X + \mu Y] &= \sum_{\omega\in \Omega} (\lambda X(\omega) + \mu Y(\omega))\mathbb{P}(\{\omega\})\\
                &= \lambda \sum_{\omega\in\Omega} X(\omega)\mathbb{P}(\{\omega\}) + \mu\sum_{\omega\in\Omega} Y(\omega)\mathbb{P}(\{\omega\})\\
                &= \lambda \mathbb{E}[X] + \mu\mathbb{E}[Y].
            \end{align*}
        \end{proof}
    \end{enumerate}
    Note that property (2) is a special case of property (3). Similarly, it extends to \(n\) RVs. It is called \textit{linearity of expectation}.
\end{enumerate}
\end{property}
\begin{remark}
    \begin{enumerate}
        \item Independence of not a condition.
    \end{enumerate}
\end{remark}